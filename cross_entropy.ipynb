{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(output, target):\n",
    "    logits = output[torch.arange(len(output)), target]\n",
    "    loss = - logits + torch.log(torch.sum(torch.exp(output), axis=-1))\n",
    "    loss = loss.mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['X1', 'X2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.X1 = np.random.randint(2, size=10**3)\n",
    "df.X2 = np.random.randint(2, size=10**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joanb\\AppData\\Local\\Temp\\ipykernel_2148\\747186736.py:1: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  a = torch.Tensor(df.X1.to_numpy())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3417.5635)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor(df.X1.to_numpy())\n",
    "b = torch.Tensor(df.X2.to_numpy())\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "loss(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. DATA: TOY EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.457633</td>\n",
       "      <td>0.274310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.170258</td>\n",
       "      <td>0.905733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.426743</td>\n",
       "      <td>0.067306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.033611</td>\n",
       "      <td>0.259964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.726409</td>\n",
       "      <td>0.747385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1        X2\n",
       "0  0.457633  0.274310\n",
       "1  0.170258  0.905733\n",
       "2  0.426743  0.067306\n",
       "3  0.033611  0.259964\n",
       "4  0.726409  0.747385"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.rand(10**3,2)\n",
    "df = pd.DataFrame(X, columns=['X1', 'X2'])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sin'] = np.sin((df.X1 + df.X2))\n",
    "df['pdiv'] = (df.X1 + df.X2)**2/(1 + (df.X1 + df.X2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>Sin</th>\n",
       "      <th>pdiv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.457633</td>\n",
       "      <td>0.274310</td>\n",
       "      <td>0.668316</td>\n",
       "      <td>0.348848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.170258</td>\n",
       "      <td>0.905733</td>\n",
       "      <td>0.880061</td>\n",
       "      <td>0.536556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.426743</td>\n",
       "      <td>0.067306</td>\n",
       "      <td>0.474195</td>\n",
       "      <td>0.196196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.033611</td>\n",
       "      <td>0.259964</td>\n",
       "      <td>0.289376</td>\n",
       "      <td>0.079348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.726409</td>\n",
       "      <td>0.747385</td>\n",
       "      <td>0.995299</td>\n",
       "      <td>0.684748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.222262</td>\n",
       "      <td>0.317026</td>\n",
       "      <td>0.513525</td>\n",
       "      <td>0.225305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.877897</td>\n",
       "      <td>0.885155</td>\n",
       "      <td>0.981576</td>\n",
       "      <td>0.756594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.313593</td>\n",
       "      <td>0.287273</td>\n",
       "      <td>0.565357</td>\n",
       "      <td>0.265267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.565696</td>\n",
       "      <td>0.173850</td>\n",
       "      <td>0.673952</td>\n",
       "      <td>0.353557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.201766</td>\n",
       "      <td>0.310177</td>\n",
       "      <td>0.489872</td>\n",
       "      <td>0.207661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1        X2       Sin      pdiv\n",
       "0  0.457633  0.274310  0.668316  0.348848\n",
       "1  0.170258  0.905733  0.880061  0.536556\n",
       "2  0.426743  0.067306  0.474195  0.196196\n",
       "3  0.033611  0.259964  0.289376  0.079348\n",
       "4  0.726409  0.747385  0.995299  0.684748\n",
       "5  0.222262  0.317026  0.513525  0.225305\n",
       "6  0.877897  0.885155  0.981576  0.756594\n",
       "7  0.313593  0.287273  0.565357  0.265267\n",
       "8  0.565696  0.173850  0.673952  0.353557\n",
       "9  0.201766  0.310177  0.489872  0.207661"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Y1'] = bernoulli.rvs(df.Sin)\n",
    "df['Y2'] = bernoulli.rvs(df.pdiv)\n",
    "df.head(10)\n",
    "df.to_csv('data_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = torch.Tensor(df[['X1', 'X2']].values)\n",
    "y1 = torch.Tensor(df[['Y1']].values)\n",
    "y2 = torch.Tensor(df[['Y2']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2,4),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(4,1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "model2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2,10),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(10,1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "\n",
    "criterion1 = torch.nn.MSELoss()\n",
    "criterion2 = torch.nn.MSELoss()\n",
    "optimizer1 = torch.optim.SGD(model1.parameters(), lr=0.8)\n",
    "optimizer2 = torch.optim.SGD(model2.parameters(), lr=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ejemplo de como funciona el gumbel softmax\n",
    "\n",
    "#F.gumbel_softmax(model(df_t).log(), hard=False)\n",
    "#model(df_t).log()\n",
    "\n",
    "logits = torch.randn(10, 5)\n",
    "F.gumbel_softmax(logits, hard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1.])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.gumbel_softmax(torch.Tensor([0.1, 0.9]).log(), tau=1, hard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        ...,\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_0 = model1(df_t)\n",
    "y_pred_1 = 1 - model1(df_t)\n",
    "\n",
    "y_pred = torch.stack((y_pred_0, y_pred_1), -1).reshape(1000,2)\n",
    "\n",
    "aux = F.gumbel_softmax(y_pred.log(), hard=True)\n",
    "aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5550, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion1(y1, aux[:, 0].reshape(1000,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/100000 Loss 0.23742\n",
      "Epoch 2000/100000 Loss 0.22647\n",
      "Epoch 3000/100000 Loss 0.22272\n",
      "Epoch 4000/100000 Loss 0.22095\n",
      "Epoch 5000/100000 Loss 0.21982\n",
      "Epoch 6000/100000 Loss 0.21909\n",
      "Epoch 7000/100000 Loss 0.21861\n",
      "Epoch 8000/100000 Loss 0.21829\n",
      "Epoch 9000/100000 Loss 0.21806\n",
      "Epoch 10000/100000 Loss 0.21784\n",
      "Epoch 11000/100000 Loss 0.21766\n",
      "Epoch 12000/100000 Loss 0.21749\n",
      "Epoch 13000/100000 Loss 0.21738\n",
      "Epoch 14000/100000 Loss 0.21728\n",
      "Epoch 15000/100000 Loss 0.21716\n",
      "Epoch 16000/100000 Loss 0.21709\n",
      "Epoch 17000/100000 Loss 0.21703\n",
      "Epoch 18000/100000 Loss 0.21694\n",
      "Epoch 19000/100000 Loss 0.21689\n",
      "Epoch 20000/100000 Loss 0.21682\n",
      "Epoch 21000/100000 Loss 0.21678\n",
      "Epoch 22000/100000 Loss 0.21672\n",
      "Epoch 23000/100000 Loss 0.21667\n",
      "Epoch 24000/100000 Loss 0.21665\n",
      "Epoch 25000/100000 Loss 0.21660\n",
      "Epoch 26000/100000 Loss 0.21657\n",
      "Epoch 27000/100000 Loss 0.21656\n",
      "Epoch 28000/100000 Loss 0.21653\n",
      "Epoch 29000/100000 Loss 0.21651\n",
      "Epoch 30000/100000 Loss 0.21649\n",
      "Epoch 31000/100000 Loss 0.21648\n",
      "Epoch 32000/100000 Loss 0.21647\n",
      "Epoch 33000/100000 Loss 0.21644\n",
      "Epoch 34000/100000 Loss 0.21642\n",
      "Epoch 35000/100000 Loss 0.21638\n",
      "Epoch 36000/100000 Loss 0.21636\n",
      "Epoch 37000/100000 Loss 0.21633\n",
      "Epoch 38000/100000 Loss 0.21632\n",
      "Epoch 39000/100000 Loss 0.21630\n",
      "Epoch 40000/100000 Loss 0.21628\n",
      "Epoch 41000/100000 Loss 0.21627\n",
      "Epoch 42000/100000 Loss 0.21625\n",
      "Epoch 43000/100000 Loss 0.21623\n",
      "Epoch 44000/100000 Loss 0.21622\n",
      "Epoch 45000/100000 Loss 0.21620\n",
      "Epoch 46000/100000 Loss 0.21619\n",
      "Epoch 47000/100000 Loss 0.21619\n",
      "Epoch 48000/100000 Loss 0.21617\n",
      "Epoch 49000/100000 Loss 0.21617\n",
      "Epoch 50000/100000 Loss 0.21616\n",
      "Epoch 51000/100000 Loss 0.21614\n",
      "Epoch 52000/100000 Loss 0.21614\n",
      "Epoch 53000/100000 Loss 0.21613\n",
      "Epoch 54000/100000 Loss 0.21612\n",
      "Epoch 55000/100000 Loss 0.21611\n",
      "Epoch 56000/100000 Loss 0.21609\n",
      "Epoch 57000/100000 Loss 0.21607\n",
      "Epoch 58000/100000 Loss 0.21607\n",
      "Epoch 59000/100000 Loss 0.21607\n",
      "Epoch 60000/100000 Loss 0.21606\n",
      "Epoch 61000/100000 Loss 0.21605\n",
      "Epoch 62000/100000 Loss 0.21604\n",
      "Epoch 63000/100000 Loss 0.21604\n",
      "Epoch 64000/100000 Loss 0.21604\n",
      "Epoch 65000/100000 Loss 0.21603\n",
      "Epoch 66000/100000 Loss 0.21603\n",
      "Epoch 67000/100000 Loss 0.21603\n",
      "Epoch 68000/100000 Loss 0.21604\n",
      "Epoch 69000/100000 Loss 0.21603\n",
      "Epoch 70000/100000 Loss 0.21603\n",
      "Epoch 71000/100000 Loss 0.21601\n",
      "Epoch 72000/100000 Loss 0.21601\n",
      "Epoch 73000/100000 Loss 0.21600\n",
      "Epoch 74000/100000 Loss 0.21600\n",
      "Epoch 75000/100000 Loss 0.21600\n",
      "Epoch 76000/100000 Loss 0.21599\n",
      "Epoch 77000/100000 Loss 0.21599\n",
      "Epoch 78000/100000 Loss 0.21599\n",
      "Epoch 79000/100000 Loss 0.21598\n",
      "Epoch 80000/100000 Loss 0.21598\n",
      "Epoch 81000/100000 Loss 0.21597\n",
      "Epoch 82000/100000 Loss 0.21597\n",
      "Epoch 83000/100000 Loss 0.21596\n",
      "Epoch 84000/100000 Loss 0.21596\n",
      "Epoch 85000/100000 Loss 0.21596\n",
      "Epoch 86000/100000 Loss 0.21596\n",
      "Epoch 87000/100000 Loss 0.21596\n",
      "Epoch 88000/100000 Loss 0.21595\n",
      "Epoch 89000/100000 Loss 0.21595\n",
      "Epoch 90000/100000 Loss 0.21595\n",
      "Epoch 91000/100000 Loss 0.21595\n",
      "Epoch 92000/100000 Loss 0.21595\n",
      "Epoch 93000/100000 Loss 0.21594\n",
      "Epoch 94000/100000 Loss 0.21594\n",
      "Epoch 95000/100000 Loss 0.21594\n",
      "Epoch 96000/100000 Loss 0.21593\n",
      "Epoch 97000/100000 Loss 0.21593\n",
      "Epoch 98000/100000 Loss 0.21593\n",
      "Epoch 99000/100000 Loss 0.21592\n",
      "Epoch 100000/100000 Loss 0.21592\n"
     ]
    }
   ],
   "source": [
    "epochs = 10**5\n",
    "log_each = 1000\n",
    "l = []\n",
    "model1.train()\n",
    "\n",
    "for e in range(1, epochs + 1):\n",
    "    y_pred_0 = model1(df_t)\n",
    "    y_pred_1 = 1 - model1(df_t)\n",
    "\n",
    "    y_pred = torch.stack((y_pred_0, y_pred_1), -1).reshape(1000,2)\n",
    "\n",
    "    y_pred = F.gumbel_softmax(y_pred.log(), hard=True)\n",
    "\n",
    "    loss = criterion1(y_pred[:, 0].reshape(1000, 1), y1)\n",
    "    l.append(loss.item())\n",
    "\n",
    "    optimizer1.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer1.step()\n",
    "\n",
    "    if not e % log_each:\n",
    "        print(f'Epoch {e}/{epochs} Loss {np.mean(l):.5f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat1 = np.zeros(model1(df_t).shape)\n",
    "\n",
    "cont = 0\n",
    "for value in model1(df_t):\n",
    "    y_hat1[cont] = value.item()\n",
    "    cont +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x191918bc340>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(df.X1.values + df.X2.values, np.sin(df.X1.values + df.X2.values))\n",
    "plt.scatter(df.X1.values + df.X2.values, y_hat1, cmap='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        ...,\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_0 = model2(df_t)\n",
    "y_pred_1 = 1 - model2(df_t)\n",
    "\n",
    "y_pred = torch.stack((y_pred_0, y_pred_1), -1).reshape(1000,2)\n",
    "y_pred = F.gumbel_softmax(y_pred.log(), hard=True)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5150, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = criterion2(y_pred[:, 0].reshape(1000, 1), y2)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer2 = torch.optim.SGD(model2.parameters(), lr=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/400000 Loss 0.43707\n",
      "Epoch 2000/400000 Loss 0.41558\n",
      "Epoch 3000/400000 Loss 0.40798\n",
      "Epoch 4000/400000 Loss 0.40377\n",
      "Epoch 5000/400000 Loss 0.40142\n",
      "Epoch 6000/400000 Loss 0.39941\n",
      "Epoch 7000/400000 Loss 0.39776\n",
      "Epoch 8000/400000 Loss 0.39624\n",
      "Epoch 9000/400000 Loss 0.39485\n",
      "Epoch 10000/400000 Loss 0.39363\n",
      "Epoch 11000/400000 Loss 0.39254\n",
      "Epoch 12000/400000 Loss 0.39155\n",
      "Epoch 13000/400000 Loss 0.39067\n",
      "Epoch 14000/400000 Loss 0.38991\n",
      "Epoch 15000/400000 Loss 0.38926\n",
      "Epoch 16000/400000 Loss 0.38861\n",
      "Epoch 17000/400000 Loss 0.38811\n",
      "Epoch 18000/400000 Loss 0.38767\n",
      "Epoch 19000/400000 Loss 0.38724\n",
      "Epoch 20000/400000 Loss 0.38688\n",
      "Epoch 21000/400000 Loss 0.38656\n",
      "Epoch 22000/400000 Loss 0.38624\n",
      "Epoch 23000/400000 Loss 0.38597\n",
      "Epoch 24000/400000 Loss 0.38574\n",
      "Epoch 25000/400000 Loss 0.38550\n",
      "Epoch 26000/400000 Loss 0.38529\n",
      "Epoch 27000/400000 Loss 0.38509\n",
      "Epoch 28000/400000 Loss 0.38489\n",
      "Epoch 29000/400000 Loss 0.38469\n",
      "Epoch 30000/400000 Loss 0.38453\n",
      "Epoch 31000/400000 Loss 0.38438\n",
      "Epoch 32000/400000 Loss 0.38424\n",
      "Epoch 33000/400000 Loss 0.38410\n",
      "Epoch 34000/400000 Loss 0.38397\n",
      "Epoch 35000/400000 Loss 0.38384\n",
      "Epoch 36000/400000 Loss 0.38373\n",
      "Epoch 37000/400000 Loss 0.38361\n",
      "Epoch 38000/400000 Loss 0.38349\n",
      "Epoch 39000/400000 Loss 0.38338\n",
      "Epoch 40000/400000 Loss 0.38329\n",
      "Epoch 41000/400000 Loss 0.38319\n",
      "Epoch 42000/400000 Loss 0.38310\n",
      "Epoch 43000/400000 Loss 0.38300\n",
      "Epoch 44000/400000 Loss 0.38291\n",
      "Epoch 45000/400000 Loss 0.38283\n",
      "Epoch 46000/400000 Loss 0.38275\n",
      "Epoch 47000/400000 Loss 0.38267\n",
      "Epoch 48000/400000 Loss 0.38261\n",
      "Epoch 49000/400000 Loss 0.38255\n",
      "Epoch 50000/400000 Loss 0.38249\n",
      "Epoch 51000/400000 Loss 0.38243\n",
      "Epoch 52000/400000 Loss 0.38236\n",
      "Epoch 53000/400000 Loss 0.38230\n",
      "Epoch 54000/400000 Loss 0.38225\n",
      "Epoch 55000/400000 Loss 0.38219\n",
      "Epoch 56000/400000 Loss 0.38213\n",
      "Epoch 57000/400000 Loss 0.38209\n",
      "Epoch 58000/400000 Loss 0.38203\n",
      "Epoch 59000/400000 Loss 0.38198\n",
      "Epoch 60000/400000 Loss 0.38192\n",
      "Epoch 61000/400000 Loss 0.38188\n",
      "Epoch 62000/400000 Loss 0.38184\n",
      "Epoch 63000/400000 Loss 0.38179\n",
      "Epoch 64000/400000 Loss 0.38175\n",
      "Epoch 65000/400000 Loss 0.38171\n",
      "Epoch 66000/400000 Loss 0.38168\n",
      "Epoch 67000/400000 Loss 0.38163\n",
      "Epoch 68000/400000 Loss 0.38160\n",
      "Epoch 69000/400000 Loss 0.38157\n",
      "Epoch 70000/400000 Loss 0.38152\n",
      "Epoch 71000/400000 Loss 0.38149\n",
      "Epoch 72000/400000 Loss 0.38147\n",
      "Epoch 73000/400000 Loss 0.38144\n",
      "Epoch 74000/400000 Loss 0.38140\n",
      "Epoch 75000/400000 Loss 0.38136\n",
      "Epoch 76000/400000 Loss 0.38133\n",
      "Epoch 77000/400000 Loss 0.38130\n",
      "Epoch 78000/400000 Loss 0.38127\n",
      "Epoch 79000/400000 Loss 0.38124\n",
      "Epoch 80000/400000 Loss 0.38122\n",
      "Epoch 81000/400000 Loss 0.38119\n",
      "Epoch 82000/400000 Loss 0.38116\n",
      "Epoch 83000/400000 Loss 0.38112\n",
      "Epoch 84000/400000 Loss 0.38110\n",
      "Epoch 85000/400000 Loss 0.38107\n",
      "Epoch 86000/400000 Loss 0.38104\n",
      "Epoch 87000/400000 Loss 0.38102\n",
      "Epoch 88000/400000 Loss 0.38099\n",
      "Epoch 89000/400000 Loss 0.38096\n",
      "Epoch 90000/400000 Loss 0.38094\n",
      "Epoch 91000/400000 Loss 0.38091\n",
      "Epoch 92000/400000 Loss 0.38088\n",
      "Epoch 93000/400000 Loss 0.38086\n",
      "Epoch 94000/400000 Loss 0.38083\n",
      "Epoch 95000/400000 Loss 0.38081\n",
      "Epoch 96000/400000 Loss 0.38079\n",
      "Epoch 97000/400000 Loss 0.38077\n",
      "Epoch 98000/400000 Loss 0.38075\n",
      "Epoch 99000/400000 Loss 0.38073\n",
      "Epoch 100000/400000 Loss 0.38071\n",
      "Epoch 101000/400000 Loss 0.38069\n",
      "Epoch 102000/400000 Loss 0.38066\n",
      "Epoch 103000/400000 Loss 0.38065\n",
      "Epoch 104000/400000 Loss 0.38063\n",
      "Epoch 105000/400000 Loss 0.38061\n",
      "Epoch 106000/400000 Loss 0.38060\n",
      "Epoch 107000/400000 Loss 0.38058\n",
      "Epoch 108000/400000 Loss 0.38056\n",
      "Epoch 109000/400000 Loss 0.38054\n",
      "Epoch 110000/400000 Loss 0.38051\n",
      "Epoch 111000/400000 Loss 0.38049\n",
      "Epoch 112000/400000 Loss 0.38047\n",
      "Epoch 113000/400000 Loss 0.38046\n",
      "Epoch 114000/400000 Loss 0.38044\n",
      "Epoch 115000/400000 Loss 0.38043\n",
      "Epoch 116000/400000 Loss 0.38041\n",
      "Epoch 117000/400000 Loss 0.38040\n",
      "Epoch 118000/400000 Loss 0.38038\n",
      "Epoch 119000/400000 Loss 0.38036\n",
      "Epoch 120000/400000 Loss 0.38035\n",
      "Epoch 121000/400000 Loss 0.38034\n",
      "Epoch 122000/400000 Loss 0.38033\n",
      "Epoch 123000/400000 Loss 0.38031\n",
      "Epoch 124000/400000 Loss 0.38030\n",
      "Epoch 125000/400000 Loss 0.38029\n",
      "Epoch 126000/400000 Loss 0.38027\n",
      "Epoch 127000/400000 Loss 0.38026\n",
      "Epoch 128000/400000 Loss 0.38025\n",
      "Epoch 129000/400000 Loss 0.38024\n",
      "Epoch 130000/400000 Loss 0.38023\n",
      "Epoch 131000/400000 Loss 0.38022\n",
      "Epoch 132000/400000 Loss 0.38021\n",
      "Epoch 133000/400000 Loss 0.38020\n",
      "Epoch 134000/400000 Loss 0.38019\n",
      "Epoch 135000/400000 Loss 0.38017\n",
      "Epoch 136000/400000 Loss 0.38016\n",
      "Epoch 137000/400000 Loss 0.38015\n",
      "Epoch 138000/400000 Loss 0.38014\n",
      "Epoch 139000/400000 Loss 0.38012\n",
      "Epoch 140000/400000 Loss 0.38012\n",
      "Epoch 141000/400000 Loss 0.38011\n",
      "Epoch 142000/400000 Loss 0.38010\n",
      "Epoch 143000/400000 Loss 0.38008\n",
      "Epoch 144000/400000 Loss 0.38007\n",
      "Epoch 145000/400000 Loss 0.38007\n",
      "Epoch 146000/400000 Loss 0.38005\n",
      "Epoch 147000/400000 Loss 0.38004\n",
      "Epoch 148000/400000 Loss 0.38003\n",
      "Epoch 149000/400000 Loss 0.38003\n",
      "Epoch 150000/400000 Loss 0.38001\n",
      "Epoch 151000/400000 Loss 0.38000\n",
      "Epoch 152000/400000 Loss 0.37999\n",
      "Epoch 153000/400000 Loss 0.37998\n",
      "Epoch 154000/400000 Loss 0.37997\n",
      "Epoch 155000/400000 Loss 0.37997\n",
      "Epoch 156000/400000 Loss 0.37996\n",
      "Epoch 157000/400000 Loss 0.37995\n",
      "Epoch 158000/400000 Loss 0.37994\n",
      "Epoch 159000/400000 Loss 0.37993\n",
      "Epoch 160000/400000 Loss 0.37992\n",
      "Epoch 161000/400000 Loss 0.37991\n",
      "Epoch 162000/400000 Loss 0.37990\n",
      "Epoch 163000/400000 Loss 0.37989\n",
      "Epoch 164000/400000 Loss 0.37988\n",
      "Epoch 165000/400000 Loss 0.37988\n",
      "Epoch 166000/400000 Loss 0.37987\n",
      "Epoch 167000/400000 Loss 0.37986\n",
      "Epoch 168000/400000 Loss 0.37986\n",
      "Epoch 169000/400000 Loss 0.37985\n",
      "Epoch 170000/400000 Loss 0.37984\n",
      "Epoch 171000/400000 Loss 0.37984\n",
      "Epoch 172000/400000 Loss 0.37983\n",
      "Epoch 173000/400000 Loss 0.37982\n",
      "Epoch 174000/400000 Loss 0.37982\n",
      "Epoch 175000/400000 Loss 0.37981\n",
      "Epoch 176000/400000 Loss 0.37980\n",
      "Epoch 177000/400000 Loss 0.37979\n",
      "Epoch 178000/400000 Loss 0.37978\n",
      "Epoch 179000/400000 Loss 0.37978\n",
      "Epoch 180000/400000 Loss 0.37977\n",
      "Epoch 181000/400000 Loss 0.37976\n",
      "Epoch 182000/400000 Loss 0.37975\n",
      "Epoch 183000/400000 Loss 0.37975\n",
      "Epoch 184000/400000 Loss 0.37974\n",
      "Epoch 185000/400000 Loss 0.37974\n",
      "Epoch 186000/400000 Loss 0.37974\n",
      "Epoch 187000/400000 Loss 0.37973\n",
      "Epoch 188000/400000 Loss 0.37972\n",
      "Epoch 189000/400000 Loss 0.37972\n",
      "Epoch 190000/400000 Loss 0.37972\n",
      "Epoch 191000/400000 Loss 0.37971\n",
      "Epoch 192000/400000 Loss 0.37970\n",
      "Epoch 193000/400000 Loss 0.37969\n",
      "Epoch 194000/400000 Loss 0.37969\n",
      "Epoch 195000/400000 Loss 0.37968\n",
      "Epoch 196000/400000 Loss 0.37968\n",
      "Epoch 197000/400000 Loss 0.37967\n",
      "Epoch 198000/400000 Loss 0.37966\n",
      "Epoch 199000/400000 Loss 0.37966\n",
      "Epoch 200000/400000 Loss 0.37966\n",
      "Epoch 201000/400000 Loss 0.37965\n",
      "Epoch 202000/400000 Loss 0.37965\n",
      "Epoch 203000/400000 Loss 0.37964\n",
      "Epoch 204000/400000 Loss 0.37964\n",
      "Epoch 205000/400000 Loss 0.37964\n",
      "Epoch 206000/400000 Loss 0.37963\n",
      "Epoch 207000/400000 Loss 0.37962\n",
      "Epoch 208000/400000 Loss 0.37962\n",
      "Epoch 209000/400000 Loss 0.37961\n",
      "Epoch 210000/400000 Loss 0.37961\n",
      "Epoch 211000/400000 Loss 0.37960\n",
      "Epoch 212000/400000 Loss 0.37960\n",
      "Epoch 213000/400000 Loss 0.37959\n",
      "Epoch 214000/400000 Loss 0.37959\n",
      "Epoch 215000/400000 Loss 0.37958\n",
      "Epoch 216000/400000 Loss 0.37958\n",
      "Epoch 217000/400000 Loss 0.37957\n",
      "Epoch 218000/400000 Loss 0.37957\n",
      "Epoch 219000/400000 Loss 0.37956\n",
      "Epoch 220000/400000 Loss 0.37956\n",
      "Epoch 221000/400000 Loss 0.37955\n",
      "Epoch 222000/400000 Loss 0.37955\n",
      "Epoch 223000/400000 Loss 0.37954\n",
      "Epoch 224000/400000 Loss 0.37954\n",
      "Epoch 225000/400000 Loss 0.37954\n",
      "Epoch 226000/400000 Loss 0.37953\n",
      "Epoch 227000/400000 Loss 0.37953\n",
      "Epoch 228000/400000 Loss 0.37952\n",
      "Epoch 229000/400000 Loss 0.37952\n",
      "Epoch 230000/400000 Loss 0.37952\n",
      "Epoch 231000/400000 Loss 0.37951\n",
      "Epoch 232000/400000 Loss 0.37951\n",
      "Epoch 233000/400000 Loss 0.37950\n",
      "Epoch 234000/400000 Loss 0.37950\n",
      "Epoch 235000/400000 Loss 0.37949\n",
      "Epoch 236000/400000 Loss 0.37949\n",
      "Epoch 237000/400000 Loss 0.37949\n",
      "Epoch 238000/400000 Loss 0.37949\n",
      "Epoch 239000/400000 Loss 0.37948\n",
      "Epoch 240000/400000 Loss 0.37948\n",
      "Epoch 241000/400000 Loss 0.37948\n",
      "Epoch 242000/400000 Loss 0.37947\n",
      "Epoch 243000/400000 Loss 0.37947\n",
      "Epoch 244000/400000 Loss 0.37947\n",
      "Epoch 245000/400000 Loss 0.37947\n",
      "Epoch 246000/400000 Loss 0.37946\n",
      "Epoch 247000/400000 Loss 0.37946\n",
      "Epoch 248000/400000 Loss 0.37945\n",
      "Epoch 249000/400000 Loss 0.37946\n",
      "Epoch 250000/400000 Loss 0.37945\n",
      "Epoch 251000/400000 Loss 0.37945\n",
      "Epoch 252000/400000 Loss 0.37945\n",
      "Epoch 253000/400000 Loss 0.37944\n",
      "Epoch 254000/400000 Loss 0.37944\n",
      "Epoch 255000/400000 Loss 0.37944\n",
      "Epoch 256000/400000 Loss 0.37943\n",
      "Epoch 257000/400000 Loss 0.37943\n",
      "Epoch 258000/400000 Loss 0.37943\n",
      "Epoch 259000/400000 Loss 0.37943\n",
      "Epoch 260000/400000 Loss 0.37942\n",
      "Epoch 261000/400000 Loss 0.37942\n",
      "Epoch 262000/400000 Loss 0.37941\n",
      "Epoch 263000/400000 Loss 0.37941\n",
      "Epoch 264000/400000 Loss 0.37941\n",
      "Epoch 265000/400000 Loss 0.37940\n",
      "Epoch 266000/400000 Loss 0.37940\n",
      "Epoch 267000/400000 Loss 0.37940\n",
      "Epoch 268000/400000 Loss 0.37939\n",
      "Epoch 269000/400000 Loss 0.37939\n",
      "Epoch 270000/400000 Loss 0.37938\n",
      "Epoch 271000/400000 Loss 0.37938\n",
      "Epoch 272000/400000 Loss 0.37938\n",
      "Epoch 273000/400000 Loss 0.37937\n",
      "Epoch 274000/400000 Loss 0.37937\n",
      "Epoch 275000/400000 Loss 0.37937\n",
      "Epoch 276000/400000 Loss 0.37937\n",
      "Epoch 277000/400000 Loss 0.37937\n",
      "Epoch 278000/400000 Loss 0.37936\n",
      "Epoch 279000/400000 Loss 0.37936\n",
      "Epoch 280000/400000 Loss 0.37936\n",
      "Epoch 281000/400000 Loss 0.37936\n",
      "Epoch 282000/400000 Loss 0.37935\n",
      "Epoch 283000/400000 Loss 0.37935\n",
      "Epoch 284000/400000 Loss 0.37934\n",
      "Epoch 285000/400000 Loss 0.37934\n",
      "Epoch 286000/400000 Loss 0.37934\n",
      "Epoch 287000/400000 Loss 0.37934\n",
      "Epoch 288000/400000 Loss 0.37934\n",
      "Epoch 289000/400000 Loss 0.37934\n",
      "Epoch 290000/400000 Loss 0.37933\n",
      "Epoch 291000/400000 Loss 0.37933\n",
      "Epoch 292000/400000 Loss 0.37933\n",
      "Epoch 293000/400000 Loss 0.37933\n",
      "Epoch 294000/400000 Loss 0.37933\n",
      "Epoch 295000/400000 Loss 0.37932\n",
      "Epoch 296000/400000 Loss 0.37932\n",
      "Epoch 297000/400000 Loss 0.37932\n",
      "Epoch 298000/400000 Loss 0.37931\n",
      "Epoch 299000/400000 Loss 0.37931\n",
      "Epoch 300000/400000 Loss 0.37931\n",
      "Epoch 301000/400000 Loss 0.37931\n",
      "Epoch 302000/400000 Loss 0.37931\n",
      "Epoch 303000/400000 Loss 0.37930\n",
      "Epoch 304000/400000 Loss 0.37930\n",
      "Epoch 305000/400000 Loss 0.37930\n",
      "Epoch 306000/400000 Loss 0.37929\n",
      "Epoch 307000/400000 Loss 0.37929\n",
      "Epoch 308000/400000 Loss 0.37929\n",
      "Epoch 309000/400000 Loss 0.37929\n",
      "Epoch 310000/400000 Loss 0.37929\n",
      "Epoch 311000/400000 Loss 0.37928\n",
      "Epoch 312000/400000 Loss 0.37928\n",
      "Epoch 313000/400000 Loss 0.37928\n",
      "Epoch 314000/400000 Loss 0.37928\n",
      "Epoch 315000/400000 Loss 0.37927\n",
      "Epoch 316000/400000 Loss 0.37927\n",
      "Epoch 317000/400000 Loss 0.37927\n",
      "Epoch 318000/400000 Loss 0.37927\n",
      "Epoch 319000/400000 Loss 0.37927\n",
      "Epoch 320000/400000 Loss 0.37927\n",
      "Epoch 321000/400000 Loss 0.37926\n",
      "Epoch 322000/400000 Loss 0.37926\n",
      "Epoch 323000/400000 Loss 0.37926\n",
      "Epoch 324000/400000 Loss 0.37926\n",
      "Epoch 325000/400000 Loss 0.37925\n",
      "Epoch 326000/400000 Loss 0.37925\n",
      "Epoch 327000/400000 Loss 0.37925\n",
      "Epoch 328000/400000 Loss 0.37925\n",
      "Epoch 329000/400000 Loss 0.37925\n",
      "Epoch 330000/400000 Loss 0.37924\n",
      "Epoch 331000/400000 Loss 0.37924\n",
      "Epoch 332000/400000 Loss 0.37924\n",
      "Epoch 333000/400000 Loss 0.37924\n",
      "Epoch 334000/400000 Loss 0.37924\n",
      "Epoch 335000/400000 Loss 0.37923\n",
      "Epoch 336000/400000 Loss 0.37923\n",
      "Epoch 337000/400000 Loss 0.37923\n",
      "Epoch 338000/400000 Loss 0.37923\n",
      "Epoch 339000/400000 Loss 0.37923\n",
      "Epoch 340000/400000 Loss 0.37923\n",
      "Epoch 341000/400000 Loss 0.37923\n",
      "Epoch 342000/400000 Loss 0.37923\n",
      "Epoch 343000/400000 Loss 0.37922\n",
      "Epoch 344000/400000 Loss 0.37922\n",
      "Epoch 345000/400000 Loss 0.37922\n",
      "Epoch 346000/400000 Loss 0.37922\n",
      "Epoch 347000/400000 Loss 0.37922\n",
      "Epoch 348000/400000 Loss 0.37922\n",
      "Epoch 349000/400000 Loss 0.37921\n",
      "Epoch 350000/400000 Loss 0.37921\n",
      "Epoch 351000/400000 Loss 0.37921\n",
      "Epoch 352000/400000 Loss 0.37921\n",
      "Epoch 353000/400000 Loss 0.37921\n",
      "Epoch 354000/400000 Loss 0.37921\n",
      "Epoch 355000/400000 Loss 0.37921\n",
      "Epoch 356000/400000 Loss 0.37921\n",
      "Epoch 357000/400000 Loss 0.37921\n",
      "Epoch 358000/400000 Loss 0.37921\n",
      "Epoch 359000/400000 Loss 0.37921\n",
      "Epoch 360000/400000 Loss 0.37920\n",
      "Epoch 361000/400000 Loss 0.37920\n",
      "Epoch 362000/400000 Loss 0.37920\n",
      "Epoch 363000/400000 Loss 0.37920\n",
      "Epoch 364000/400000 Loss 0.37920\n",
      "Epoch 365000/400000 Loss 0.37920\n",
      "Epoch 366000/400000 Loss 0.37920\n",
      "Epoch 367000/400000 Loss 0.37919\n",
      "Epoch 368000/400000 Loss 0.37919\n",
      "Epoch 369000/400000 Loss 0.37919\n",
      "Epoch 370000/400000 Loss 0.37919\n",
      "Epoch 371000/400000 Loss 0.37919\n",
      "Epoch 372000/400000 Loss 0.37919\n",
      "Epoch 373000/400000 Loss 0.37919\n",
      "Epoch 374000/400000 Loss 0.37919\n",
      "Epoch 375000/400000 Loss 0.37919\n",
      "Epoch 376000/400000 Loss 0.37918\n",
      "Epoch 377000/400000 Loss 0.37918\n",
      "Epoch 378000/400000 Loss 0.37918\n",
      "Epoch 379000/400000 Loss 0.37918\n",
      "Epoch 380000/400000 Loss 0.37918\n",
      "Epoch 381000/400000 Loss 0.37917\n",
      "Epoch 382000/400000 Loss 0.37917\n",
      "Epoch 383000/400000 Loss 0.37917\n",
      "Epoch 384000/400000 Loss 0.37917\n",
      "Epoch 385000/400000 Loss 0.37917\n",
      "Epoch 386000/400000 Loss 0.37917\n",
      "Epoch 387000/400000 Loss 0.37917\n",
      "Epoch 388000/400000 Loss 0.37917\n",
      "Epoch 389000/400000 Loss 0.37917\n",
      "Epoch 390000/400000 Loss 0.37917\n",
      "Epoch 391000/400000 Loss 0.37917\n",
      "Epoch 392000/400000 Loss 0.37917\n",
      "Epoch 393000/400000 Loss 0.37917\n",
      "Epoch 394000/400000 Loss 0.37916\n",
      "Epoch 395000/400000 Loss 0.37916\n",
      "Epoch 396000/400000 Loss 0.37916\n",
      "Epoch 397000/400000 Loss 0.37916\n",
      "Epoch 398000/400000 Loss 0.37916\n",
      "Epoch 399000/400000 Loss 0.37916\n",
      "Epoch 400000/400000 Loss 0.37915\n"
     ]
    }
   ],
   "source": [
    "epochs = 4*10**5\n",
    "log_each = 1000\n",
    "l = []\n",
    "model2.train()\n",
    "\n",
    "for e in range(1, epochs + 1):\n",
    "    y_pred_0 = model2(df_t)\n",
    "    y_pred_1 = 1 - model2(df_t)\n",
    "\n",
    "    y_pred = torch.stack((y_pred_0, y_pred_1), -1).reshape(1000,2)\n",
    "\n",
    "    y_pred = F.gumbel_softmax(y_pred.log(), hard=True)\n",
    "\n",
    "    loss = criterion2(y_pred[:, 0].reshape(1000, 1), y2)\n",
    "    l.append(loss.item())\n",
    "\n",
    "    optimizer2.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer2.step()\n",
    "\n",
    "    if not e % log_each:\n",
    "        print(f'Epoch {e}/{epochs} Loss {np.mean(l):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1919c1ccbe0>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_hat2 = np.zeros(model2(df_t).shape)\n",
    "\n",
    "cont = 0\n",
    "for value in model2(df_t):\n",
    "    y_hat2[cont] = value.item()\n",
    "    cont +=1\n",
    "x = df.X1.values + df.X2.values    \n",
    "plt.scatter(x, x/(1 + x))\n",
    "plt.scatter(x, y_hat2, cmap='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Usando Monte-Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2,4),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(4,1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "model2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2,10),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(10,1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "\n",
    "criterion1 = torch.nn.MSELoss()\n",
    "criterion2 = torch.nn.MSELoss()\n",
    "optimizer1 = torch.optim.SGD(model1.parameters(), lr=0.8)\n",
    "optimizer2 = torch.optim.SGD(model2.parameters(), lr=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0., 5000.],\n",
       "        [   0., 5000.],\n",
       "        [5000.,    0.],\n",
       "        ...,\n",
       "        [5000.,    0.],\n",
       "        [   0., 5000.],\n",
       "        [   0., 5000.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 10**2\n",
    "y_pred_0 = model1(df_t)\n",
    "y_pred_1 = 1 - model1(df_t)\n",
    "\n",
    "y_pred = torch.stack((y_pred_0, y_pred_1), -1).reshape(1000,2)\n",
    "\n",
    "y_pred = F.gumbel_softmax(y_pred.log(), hard=True)\n",
    "\n",
    "for i in range(1, N):\n",
    "    y_pred += F.gumbel_softmax(y_pred.log(), hard=True)\n",
    "\n",
    "y_pred = y_pred/2*N\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1000, 2]], which is output 0 of AddBackward0, is at version 99; expected version 98 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\joanb\\Desktop\\PyTorch\\cross_entropy.ipynb Celda 29\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/joanb/Desktop/PyTorch/cross_entropy.ipynb#X63sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m l\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/joanb/Desktop/PyTorch/cross_entropy.ipynb#X63sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m optimizer1\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/joanb/Desktop/PyTorch/cross_entropy.ipynb#X63sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/joanb/Desktop/PyTorch/cross_entropy.ipynb#X63sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m optimizer1\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/joanb/Desktop/PyTorch/cross_entropy.ipynb#X63sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m e \u001b[39m%\u001b[39m log_each:\n",
      "File \u001b[1;32mc:\\Users\\joanb\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\joanb\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1000, 2]], which is output 0 of AddBackward0, is at version 99; expected version 98 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "N = 10**2\n",
    "epochs = 10**4\n",
    "log_each = 100\n",
    "l = []\n",
    "model1.train()\n",
    "\n",
    "for e in range(1, epochs + 1):\n",
    "    y_pred_0 = model1(df_t)\n",
    "    y_pred_1 = 1 - model1(df_t)\n",
    "\n",
    "    y_pred = torch.stack((y_pred_0, y_pred_1), -1).reshape(1000,2)\n",
    "\n",
    "    y_pred = F.gumbel_softmax(y_pred.log(), hard=True)\n",
    "\n",
    "    for i in range(1, N):\n",
    "        y_pred += F.gumbel_softmax(y_pred.log(), hard=True)\n",
    "    \n",
    "    y_pred = y_pred/N\n",
    "\n",
    "    loss = criterion1(y_pred[:, 0].reshape(1000, 1), y1)\n",
    "    l.append(loss.item())\n",
    "\n",
    "    optimizer1.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer1.step()\n",
    "\n",
    "    if not e % log_each:\n",
    "        print(f'Epoch {e}/{epochs} Loss {np.mean(l):.5f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gumbel-Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.6489e-01, -6.6671e-01, -1.2384e-01,  1.2796e+00,  3.8441e-01,\n",
       "         -3.6823e-03,  6.3410e-01,  2.8906e-01, -1.8588e+00, -3.9752e-01,\n",
       "         -1.9323e-01,  1.7230e-01, -1.5909e+00, -1.6632e+00,  1.7779e+00,\n",
       "         -1.3180e+00,  1.5212e-01, -1.2113e+00,  1.3388e+00,  3.7228e-01,\n",
       "          1.0506e+00, -9.5167e-01, -2.0212e+00,  5.3694e-01, -4.0040e-01,\n",
       "          5.9770e-01,  2.6239e-01,  2.4333e+00, -4.5926e-02,  1.0183e+00,\n",
       "          2.4635e+00, -7.3983e-01],\n",
       "        [-2.2115e-01,  7.8617e-01, -2.6264e-03,  7.1794e-01,  2.9646e-02,\n",
       "         -2.6005e-02, -9.6798e-01,  7.4989e-01,  1.0117e+00,  2.9135e-02,\n",
       "         -2.5381e-02,  6.7920e-01, -6.7292e-01, -1.5750e+00,  4.0203e-01,\n",
       "         -4.0640e-01, -9.7063e-01, -5.1587e-01,  2.8869e-01,  6.7551e-01,\n",
       "         -7.6901e-01,  9.7987e-01, -8.6513e-01, -1.2525e-01,  1.2387e+00,\n",
       "          4.4873e-01,  1.4693e-01, -1.7503e+00, -2.0142e+00,  1.1206e+00,\n",
       "          4.3872e-01, -2.2206e-01],\n",
       "        [ 1.0507e+00,  2.1556e+00, -1.1584e+00, -1.1736e+00, -5.2536e-01,\n",
       "          2.4427e-01,  1.4645e-01, -1.0625e+00, -1.0252e+00,  1.0116e+00,\n",
       "         -1.4917e+00,  6.7432e-01,  1.4152e+00,  1.3810e+00,  5.4363e-01,\n",
       "         -1.5321e+00, -1.4719e+00,  2.4463e-01,  1.7882e+00, -4.6222e-01,\n",
       "          9.3206e-01,  1.1155e+00,  1.5648e+00,  6.3727e-01, -5.2183e-01,\n",
       "          1.2432e+00,  5.3625e-01,  9.9335e-01,  2.7267e-01, -4.4231e-02,\n",
       "         -1.0427e-01,  1.7209e-01],\n",
       "        [-8.6249e-01, -7.3821e-01,  6.8089e-01, -5.4993e-01,  1.3846e+00,\n",
       "          2.9053e-01, -1.6755e+00,  1.1349e+00,  1.5440e+00,  1.3513e+00,\n",
       "          1.2598e-01,  1.3509e+00, -1.5689e-01,  1.0009e-01,  5.2012e-01,\n",
       "         -6.1184e-01, -5.9438e-01, -3.7180e-01,  1.3469e+00,  1.5048e+00,\n",
       "         -3.4428e-01, -5.7753e-01, -9.6095e-01,  3.1048e-01, -9.7599e-01,\n",
       "         -2.9606e-01, -5.8724e-01,  1.2840e-01, -3.7274e-01, -1.7307e+00,\n",
       "          2.6770e-01, -1.7200e+00],\n",
       "        [-2.2972e+00,  1.8383e+00,  1.6341e+00, -1.0546e+00,  5.4375e-01,\n",
       "          1.4687e+00, -2.6773e+00,  1.0943e+00, -1.9235e-01,  2.3451e-01,\n",
       "         -2.9215e-01,  1.3344e+00,  3.2781e-01, -8.7064e-01, -8.6752e-01,\n",
       "         -8.5267e-01, -1.8453e-01, -1.4559e-01, -6.9129e-01, -6.0594e-02,\n",
       "          1.0921e+00,  4.8344e-01,  1.3234e+00, -4.9560e-01, -2.8547e+00,\n",
       "         -8.5051e-02, -7.8100e-01, -6.5636e-01, -4.6100e-01,  2.3685e-01,\n",
       "         -2.8276e-01, -6.4042e-01],\n",
       "        [-8.7239e-01, -5.4749e-01,  2.6218e-01,  7.6266e-01, -1.0213e+00,\n",
       "          5.5152e-01,  2.4273e-01,  1.5399e-01, -1.8522e+00,  1.0578e+00,\n",
       "         -2.8030e-01,  8.6586e-01, -2.2016e+00,  1.5186e+00,  5.3214e-01,\n",
       "          1.8554e+00, -1.1279e-01,  1.0224e+00, -8.8286e-01,  1.4597e+00,\n",
       "         -1.8047e+00, -2.7437e-02,  3.5515e-02, -4.2243e-01, -1.4176e+00,\n",
       "          2.0765e-01, -2.0736e-01, -1.7597e+00,  2.3712e-02,  1.6377e+00,\n",
       "         -1.8463e-01,  8.8067e-01],\n",
       "        [-1.8726e-01,  9.8227e-01, -8.3579e-01,  1.7805e-01, -2.0523e+00,\n",
       "          4.8886e-01,  2.3443e-01,  7.8750e-01,  5.8453e-01,  9.1586e-02,\n",
       "          4.8672e-01, -2.0219e+00,  4.6905e-01,  1.5957e+00,  4.5414e-02,\n",
       "          1.2849e+00, -9.3495e-01,  7.8083e-02, -3.2646e-01,  7.7045e-01,\n",
       "         -1.5018e+00, -5.5008e-01, -1.3371e+00,  5.7409e-01,  4.8427e-02,\n",
       "          4.0623e-01, -1.1656e-01,  1.1221e+00,  1.9070e-01,  5.7590e-01,\n",
       "          9.3994e-02,  7.7089e-01],\n",
       "        [-1.5095e+00, -9.8994e-01,  1.9732e-01, -2.7657e-01,  9.1156e-01,\n",
       "          7.9092e-01,  6.0540e-01, -1.9920e+00,  8.6664e-01, -7.0995e-02,\n",
       "          1.5836e+00, -1.7473e+00,  4.0285e-01,  5.3406e-01, -2.6857e+00,\n",
       "         -2.5733e-01, -3.6469e-01,  4.2511e-02, -6.0713e-01, -1.4068e+00,\n",
       "          1.2873e+00, -3.5174e-03,  1.5052e+00, -1.3513e+00, -3.6361e-01,\n",
       "         -8.1631e-02, -2.0209e+00,  1.2615e+00, -9.0938e-01,  1.9089e+00,\n",
       "          6.3340e-01,  8.9643e-01],\n",
       "        [-8.6517e-01,  6.0136e-01,  1.7242e+00,  3.2578e-01, -1.0677e+00,\n",
       "         -1.0424e+00,  7.9548e-01, -6.5018e-01,  3.3314e-01,  2.4355e+00,\n",
       "         -2.4919e-01,  2.2529e+00, -8.1836e-01,  1.9634e+00, -3.9620e-01,\n",
       "          6.9809e-01, -1.1176e-01, -1.1072e+00,  9.6493e-01, -6.5984e-01,\n",
       "          8.6062e-01,  1.9857e+00,  4.0304e-01,  1.1033e+00,  4.7494e-01,\n",
       "         -6.6478e-01, -3.3881e-01, -9.4571e-01,  1.1415e-01, -6.5543e-01,\n",
       "         -1.1698e-01,  2.4083e+00],\n",
       "        [ 1.8583e+00, -4.0797e-01,  9.1396e-02,  4.1125e-02, -1.4254e-01,\n",
       "          1.0305e+00,  1.4781e+00,  1.9628e-02, -7.2820e-01,  1.2349e-01,\n",
       "         -3.3256e-01, -3.6881e-01, -2.3843e-01,  6.6940e-01,  2.4801e+00,\n",
       "          2.0042e+00,  1.7709e+00,  2.4828e-01, -6.6041e-01, -1.5332e-01,\n",
       "          1.1948e+00,  1.4773e+00, -1.3509e+00, -1.7517e-01, -2.4161e-01,\n",
       "          1.5393e+00,  2.8203e+00, -2.8073e-02, -1.5314e-01, -1.1978e+00,\n",
       "         -2.5700e+00, -4.7278e-01],\n",
       "        [-1.5821e+00,  8.2318e-02, -1.7663e+00, -2.5428e-01, -1.8700e-01,\n",
       "         -1.8491e-03, -4.7862e-01, -2.1200e-01,  2.6779e+00, -1.9607e-01,\n",
       "         -2.1465e-01, -1.6938e+00, -1.4404e+00,  9.8708e-01, -8.6662e-01,\n",
       "         -1.0551e-01,  1.4209e+00, -3.9500e-01,  2.1490e+00, -1.9768e+00,\n",
       "         -5.6438e-01,  6.0095e-01,  6.7989e-01, -6.0914e-01, -9.8366e-01,\n",
       "          2.3546e-01, -1.3872e+00,  2.3510e-01,  1.3780e+00,  1.0676e+00,\n",
       "          7.6534e-01,  3.7600e-01],\n",
       "        [ 2.7436e-01,  9.1598e-01, -1.2739e-01, -1.1961e+00, -1.0680e+00,\n",
       "         -4.3545e-01, -2.5648e+00,  7.5750e-01, -7.8498e-01, -4.1582e-01,\n",
       "          1.2337e+00,  1.6417e+00, -9.5754e-01, -1.2039e+00,  1.4241e-01,\n",
       "         -3.1764e-01,  1.1916e+00, -2.2481e+00, -1.3391e+00,  5.1936e-01,\n",
       "         -1.1134e-01,  1.1512e+00, -5.7481e-01,  1.0717e+00, -2.6763e+00,\n",
       "          3.4433e-01, -1.4596e+00, -2.1648e-01, -3.2531e+00,  2.1990e+00,\n",
       "          1.2584e+00,  3.8737e-01],\n",
       "        [ 1.2232e+00, -1.5321e+00,  4.3245e-01, -4.5219e-02,  1.8970e-01,\n",
       "          5.8283e-01, -1.0834e-01, -1.5100e+00, -1.3161e-02, -4.2694e-01,\n",
       "          1.9835e+00, -7.2191e-02,  9.3410e-02,  1.7389e+00, -7.7227e-01,\n",
       "          2.6303e-01, -4.1702e-01,  9.8710e-01, -4.4406e-01, -1.2613e+00,\n",
       "          6.3639e-02, -6.9254e-01, -4.9581e-01, -3.3737e-01,  5.5508e-01,\n",
       "         -1.1876e+00,  1.3014e+00, -1.1787e+00, -1.7748e+00,  8.1642e-01,\n",
       "         -8.1540e-01,  4.3210e-01],\n",
       "        [ 1.9749e-01, -8.0670e-01, -1.8489e+00,  1.0315e+00, -1.7972e+00,\n",
       "          4.8994e-02,  5.5243e-01, -1.2738e+00,  2.0837e-01,  1.1192e+00,\n",
       "          6.3436e-01, -3.6375e-01, -2.2257e-01, -1.1628e+00,  1.3484e+00,\n",
       "         -2.7350e-02,  1.4516e-01, -1.6037e+00, -1.7397e+00,  2.8798e+00,\n",
       "         -1.8749e+00, -6.6843e-01, -1.5699e+00,  1.0375e+00, -9.4180e-02,\n",
       "         -1.4240e+00,  1.2352e+00, -6.1295e-01,  2.9175e-01,  7.4415e-01,\n",
       "         -8.4406e-01,  8.4729e-02],\n",
       "        [ 1.0773e+00, -5.5363e-01, -2.0711e-01, -1.9790e-01, -1.4223e+00,\n",
       "          1.5149e-02,  7.6480e-01,  5.4088e-01, -4.1818e-01, -1.0545e+00,\n",
       "         -2.9212e-01,  1.9651e+00,  6.8958e-01, -4.1272e-01, -1.7913e-01,\n",
       "          4.3342e-01,  5.7135e-01, -7.6946e-01,  1.0210e+00, -6.5136e-01,\n",
       "          3.6881e-01, -3.4066e-01, -3.0312e-01, -9.7437e-01, -1.1645e-01,\n",
       "         -1.8097e+00, -3.3156e-01,  2.1631e-01,  6.1072e-01,  4.4655e-01,\n",
       "          8.1267e-01,  1.6026e+00],\n",
       "        [-1.3703e+00,  7.2071e-01,  2.1988e+00,  5.9203e-01, -5.9937e-01,\n",
       "          2.9519e-01, -3.3111e-01, -1.5176e+00, -1.3149e+00,  3.5703e-01,\n",
       "         -2.4917e-01, -2.8873e-01,  4.1742e-01,  9.5301e-01, -1.2186e+00,\n",
       "         -1.7281e+00,  7.2076e-01,  8.7890e-01,  2.2806e+00,  4.0087e-01,\n",
       "         -1.3453e+00, -1.7103e+00, -7.6729e-01, -7.7511e-01,  1.0375e+00,\n",
       "          1.7472e+00, -2.0612e-01, -2.0143e-01, -4.8149e-01, -5.0080e-01,\n",
       "          1.0202e+00,  1.0015e+00],\n",
       "        [ 3.0950e-01, -5.2186e-01, -5.1413e-03,  1.8176e-01,  1.6723e+00,\n",
       "          8.2247e-01, -4.1922e-01, -4.9259e-01,  2.1062e+00,  2.9532e+00,\n",
       "         -2.3271e-01,  1.2233e+00, -1.9158e+00, -1.6328e+00,  8.4823e-02,\n",
       "          1.8085e+00, -8.9074e-01, -6.5162e-02,  9.7206e-01,  7.7859e-01,\n",
       "         -1.7175e+00, -2.5382e-01,  9.2676e-01, -9.1503e-01, -1.0100e+00,\n",
       "          1.9259e+00, -3.7004e-01, -5.5770e-01,  7.1481e-01, -9.8842e-01,\n",
       "         -4.7194e-01,  1.6704e-01],\n",
       "        [-9.2518e-01,  1.1979e+00,  1.0990e+00, -1.1142e+00,  3.8246e-02,\n",
       "          4.8234e-01,  1.9767e-01, -1.9329e-02,  5.6862e-01, -9.5541e-01,\n",
       "          5.4297e-01,  5.8467e-02, -8.5936e-01,  1.0879e+00,  1.2755e-01,\n",
       "          1.4851e+00,  1.3350e+00, -1.9732e-01, -2.7128e-02,  1.1260e+00,\n",
       "         -8.1667e-01, -2.8680e-01, -9.5683e-01,  1.4056e-01, -2.7238e-01,\n",
       "          1.5490e+00,  3.2268e+00, -5.5772e-01, -6.9635e-01,  3.1879e-01,\n",
       "         -1.6089e+00,  1.2905e-01],\n",
       "        [ 4.9506e-01, -6.1042e-01, -1.0951e+00, -3.9483e-01,  1.0769e+00,\n",
       "         -8.9606e-01,  6.0314e-01,  7.1239e-01,  1.1661e+00, -1.1573e+00,\n",
       "         -8.2801e-01, -1.3633e+00,  1.0328e-02, -1.2077e+00, -8.8500e-01,\n",
       "         -8.9158e-01, -2.2159e-01,  5.3309e-01, -1.1544e+00,  6.9685e-01,\n",
       "          1.1380e+00,  5.0587e-02,  2.2000e-01, -4.1800e-01,  1.4665e-01,\n",
       "         -3.6867e-01,  7.4228e-01,  5.4052e-01, -4.4960e-01, -9.5559e-01,\n",
       "         -6.2316e-01,  8.9492e-01],\n",
       "        [ 1.4032e+00, -1.1009e+00, -1.0984e+00, -4.1003e-01, -3.2035e+00,\n",
       "          8.6391e-01, -1.2417e+00,  2.0707e-01,  6.1515e-01,  9.3771e-01,\n",
       "         -1.0614e+00,  1.0772e+00,  8.3273e-01,  5.3341e-01, -7.8341e-01,\n",
       "          5.4786e-01,  2.3863e-01, -8.0818e-01,  1.1578e+00,  5.6152e-01,\n",
       "         -5.7069e-01,  7.0673e-01,  1.4869e+00,  7.5596e-01, -1.9414e+00,\n",
       "          9.1031e-01,  3.7552e-01, -7.7580e-01,  7.3411e-01,  1.0991e+00,\n",
       "          1.2547e-01,  1.2080e+00]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.randn(20, 32)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1.])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.gumbel_softmax(torch.Tensor([0.5, 0.5]).log(), hard=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f8852db887e4898adf41e8dfeeeb2275e3a600f3c56ac76bec9205a61cab2c77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
